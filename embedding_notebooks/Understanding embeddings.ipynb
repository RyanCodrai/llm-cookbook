{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec880bd",
   "metadata": {},
   "source": [
    "# What are embeddings?\n",
    "\n",
    "Embeddings are used to transform textual data into a format that can be easily processed by machine learning algorithms. An embedding refers to a numerical representation of words, phrases, or entire documents that captures semantic meaning and relationships between them. They are n-dimensional vectors that represent singular points in an n-dimensional space. Two points that are close together in this space are also closely related in semantic meaning.\n",
    "\n",
    "# What's so good about them?\n",
    "If we compare the use of embeddings to a bag of words approach we can immediately notice a few key benefits.\n",
    "1. Because embeddings use dense vectors they are far more memory efficient than one-hot encoded texts, even when using sparse arrays. \n",
    "2. Our ability to compare two texts does not rely on them having some or even any overlapping text. Comparing embeddings allows us to treat text as a collection of interconnected concepts rather than a set of lexical symbols. This more closely mimics how humans understand language and it's why most if not all large language models use embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e7af0",
   "metadata": {},
   "source": [
    "# Loading OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "724b1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the .env file in the parent directory\n",
    "env_path = Path(\"..\") / \".env\"\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Get the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dead356",
   "metadata": {},
   "source": [
    "# Retrieving OpenAI embeddings\n",
    "\n",
    "OpenAI's text-embedding-ada-002 model provides a fast and cost-effective method of retrieving high-performance embeddings for our text. While we could use a pre-trained model from the likes of hugging face, the performance of those embeddings on tasks such as semantic search is unlikely to perform as well as the embeddings provided by OpenAI. We would also be limited by the availability of local compute while with OpenAI we are able to make our embedding requests asyncronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e4c8f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import concurrent\n",
    "import numpy as np\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "def parallel_embedding(text_list, model=\"text-embedding-ada-002\"):\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        # Submit tasks to the executor and remember the order\n",
    "        futures = []\n",
    "        mapping = dict()\n",
    "        for i, text in enumerate(text_list):\n",
    "            future = executor.submit(get_embedding, text, model)\n",
    "            futures.append(future)\n",
    "            mapping[future] = i\n",
    "\n",
    "        # Retrieve results as they become available and sort their order\n",
    "        embeddings = [None] * len(futures)\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            embeddings[mapping[future]] = future.result()\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dbee1316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions Embeddings Shape: (5, 1536)\n",
      "Gemstones Embeddings Shape: (5, 1536)\n",
      "Fruits Embeddings Shape: (5, 1536)\n",
      "Animals Embeddings Shape: (5, 1536)\n"
     ]
    }
   ],
   "source": [
    "emotions = [\n",
    "    ['Joy', 'Happiness', 'Excitement'],\n",
    "    ['Sadness', 'Grief', 'Sorrow'],\n",
    "    ['Anger', 'Rage', 'Fury'],\n",
    "    ['Fear', 'Dread', 'Terror'],\n",
    "    ['Surprise', 'Astonishment', 'Amazement']\n",
    "]\n",
    "\n",
    "gemstones = [\n",
    "    ['Diamond', 'Radiant', 'Gem'],\n",
    "    ['Ruby', 'Garnet', 'Rhodolite'],\n",
    "    ['Sapphire', 'Topaz', 'Aquamarine'],\n",
    "    ['Emerald', 'Peridot', 'Jade'],\n",
    "    ['Amethyst', 'Quartz', 'Lavenderite']\n",
    "]\n",
    "\n",
    "fruits = [\n",
    "    ['Apple', 'Fruit', 'Crisp'],\n",
    "    ['Orange', 'Mandarin', 'Clementine'],\n",
    "    ['Banana', 'Plantain', 'Manzano'],\n",
    "    ['Kiwi', 'Fuzzy', 'Refreshing'],\n",
    "    ['Strawberry', 'Raspberry', 'Blueberry']\n",
    "]\n",
    "\n",
    "animals = [\n",
    "    ['Cat', 'Feline', 'Kitty'],\n",
    "    ['Panther', 'Jaguar', 'Leopard'],\n",
    "    ['Jaguar', 'Leopard', 'Cheetah'],\n",
    "    ['Lion', 'Lioness', 'Cub'],\n",
    "    ['Tiger', 'Bengal', 'Siberian']\n",
    "]\n",
    "\n",
    "\n",
    "def concatenate_lists(list_of_lists):\n",
    "    return [', '.join(inner_list) for inner_list in list_of_lists]\n",
    "\n",
    "emotions = concatenate_lists(emotions)\n",
    "gemstones = concatenate_lists(gemstones)\n",
    "fruits = concatenate_lists(fruits)\n",
    "animals = concatenate_lists(animals)\n",
    "\n",
    "# Retrieve the embeddings\n",
    "embeddings = parallel_embedding(emotions + gemstones + fruits + animals)\n",
    "\n",
    "# Split the embeddings for emotions, gemstones, fruits, and animals\n",
    "emotions_embeddings = embeddings[:5]\n",
    "gemstones_embeddings = embeddings[5:10]\n",
    "fruits_embeddings = embeddings[10:15]\n",
    "animals_embeddings = embeddings[15:]\n",
    "\n",
    "# Display the shape of the matrices\n",
    "print(\"Emotions Embeddings Shape:\", emotions_embeddings.shape)\n",
    "print(\"Gemstones Embeddings Shape:\", gemstones_embeddings.shape)\n",
    "print(\"Fruits Embeddings Shape:\", fruits_embeddings.shape)\n",
    "print(\"Animals Embeddings Shape:\", animals_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb4638",
   "metadata": {},
   "source": [
    "Retrieving embeddings for our emotions, gemstones, fruits and animals gives four matracies with 5 rows and 1536 columns. The 1536 columns are used to describe the position of each vector in the 1536 dimensional embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b93d7",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "As we said previously, vectors that are close in the embedding space are close in semantic meaning. To illustrate this further let's use k-means clustering to group our embedding vectors into 4 distinct clusters. We'll label our data points with the original texts that were used to create the embeddings so that we can examine which texts have been placed in which clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d85b4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Members belonging to cluster 2\n",
      "\tJoy, Happiness, Excitement\n",
      "\tSadness, Grief, Sorrow\n",
      "\tAnger, Rage, Fury\n",
      "\tFear, Dread, Terror\n",
      "\tSurprise, Astonishment, Amazement\n",
      "\n",
      "Members belonging to cluster 1\n",
      "\tDiamond, Radiant, Gem\n",
      "\tRuby, Garnet, Rhodolite\n",
      "\tSapphire, Topaz, Aquamarine\n",
      "\tEmerald, Peridot, Jade\n",
      "\tAmethyst, Quartz, Lavenderite\n",
      "\n",
      "Members belonging to cluster 3\n",
      "\tApple, Fruit, Crisp\n",
      "\tOrange, Mandarin, Clementine\n",
      "\tBanana, Plantain, Manzano\n",
      "\tKiwi, Fuzzy, Refreshing\n",
      "\tStrawberry, Raspberry, Blueberry\n",
      "\n",
      "Members belonging to cluster 0\n",
      "\tCat, Feline, Kitty\n",
      "\tPanther, Jaguar, Leopard\n",
      "\tJaguar, Leopard, Cheetah\n",
      "\tLion, Lioness, Cub\n",
      "\tTiger, Bengal, Siberian\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "\n",
    "# Combine the labels\n",
    "labels = emotions + gemstones + fruits + animals\n",
    "\n",
    "# Use K-Means Clustering\n",
    "num_clusters = 4  # Specify the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Create a dictionary to store cluster labels for each category\n",
    "cluster_dict = {label: cluster for label, cluster in zip(labels, cluster_labels)}\n",
    "\n",
    "# Collect labels into lists based on cluster\n",
    "clusters = defaultdict(list)\n",
    "for label, cluster in cluster_dict.items():\n",
    "    clusters[cluster].append(label)\n",
    "\n",
    "# Print the cluster assignments for each category\n",
    "for cluster, l in clusters.items():\n",
    "    print(f\"Members belonging to cluster {cluster}\")\n",
    "    for label in l:\n",
    "        print(f\"\\t{label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3bb05",
   "metadata": {},
   "source": [
    "The ability of this unsupervised algorithm to correctly group our input texts based on their corresponding dense vector embeddings demonstrates clearly that the text semantics have been captured succesfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf790f8",
   "metadata": {},
   "source": [
    "# Cosine similarity vs Euclidean distance\n",
    "K-means clustering uses euclidean distance which means that it attempts to find a clustering that minimises the straight line distance between members of the cluster and the cluster centroids.\n",
    "\n",
    "A more appropriate measure of similairty is cosine similarity. It has several advantages over euclidean distance for comparing texts but the main points are:\n",
    "1. Costine similarity is a measure of orientation, not magnitude. This makes it particularly suitable for text data, where the length of the vectors (i.e., the length or word count of the documents) can vary significantly, but we're more interested in the direction (i.e., the context or topic).\n",
    "\n",
    "2. Text embeddings, particularly those derived from models like Word2Vec, FastText, or BERT, can be high-dimensional. Cosine similarity works well in high-dimensional spaces where the Euclidean distance can become less meaningful due to the curse of dimensionality. The curse of dimensionality says that in high a dimensional space \"all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\"\n",
    "\n",
    "3. Cosine similarity values lie between -1 and 1 (though, in practice, for text embeddings, they often lie between 0 and 1). A value of 1 indicates that the vectors are identical in orientation, 0 indicates orthogonality (completely dissimilar), and -1 indicates opposite direction (though this negative case is rare in text embeddings). This bounded range is convenient and intuitive for many applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
