{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a3df1",
   "metadata": {},
   "source": [
    "# Near duplicate detection\n",
    "\n",
    "Near-duplicate detection in texts refers to the process of identifying documents or text segments that are almost, but not exactly, the same. These near-duplicates might differ in terms of a few words, punctuation, formatting, or minor rephrasing, but they convey very similar or identical information.\n",
    "\n",
    "Applications of near duplicate detection include:\n",
    "- Search Engines: To improve diversity of search results by filtering out duplicate pages that have substantially the same content.\n",
    "- Plagiarism Detection: To identify instances where a text might have been copied with minor modifications.\n",
    "- Data Deduplication: In large data repositories, it's important to avoid storing multiple near-identical versions of the same document. This is especially important in machine learning as duplicate texts may lead to imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c0f6b",
   "metadata": {},
   "source": [
    "# Loading OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc04092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the .env file in the parent directory\n",
    "env_path = Path(\"..\") / \".env\"\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Get the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78295334",
   "metadata": {},
   "source": [
    "#Â The dataset\n",
    "\n",
    "The dataset we'll be using comes from Kaggle and can be found [here](https://www.kaggle.com/datasets/stackoverflow/statsquestions). This dataset contains questions and answers from the cross-validated stack exchange (which is the machine learning equivilant of stack overflow). I've created a smaller version of this dataset that contains just the question titles. We'll use near-duplicate detection to find duplicate questions based on their titles. Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "064d40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../datasets/cross-validated/questions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55c822",
   "metadata": {},
   "source": [
    "# High-level approach\n",
    "To achieve near duplicate detection using text embeddings we'll:\n",
    "- Retrieve embeddings for each of texts\n",
    "- Find the n most similar segments for each segment using cosine similarity and the faiss for efficiency\n",
    "- Record which segment pairs have similarity higher than some threshold\n",
    "- Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fe573",
   "metadata": {},
   "source": [
    "# Retrieving OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec63c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import concurrent\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "def parallel_embedding(text_list, model=\"text-embedding-ada-002\"):\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        # Submit tasks to the executor and remember the order\n",
    "        futures = []\n",
    "        mapping = dict()\n",
    "        for i, text in tqdm(enumerate(text_list)):\n",
    "            future = executor.submit(get_embedding, text, model)\n",
    "            futures.append(future)\n",
    "            mapping[future] = i\n",
    "\n",
    "        # Retrieve results as they become available and sort their order\n",
    "        embeddings = [None] * len(futures)\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures)):\n",
    "            embeddings[mapping[future]] = future.result()\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d04e67",
   "metadata": {},
   "source": [
    "# Calculating costs\n",
    "Embedding text is not free - it costs $0.0001 per 1,000 tokens embedded using text-embedding-ada-002. To ensure that we don't bankrupt ourselves let's calculate the cost of making these api calls to embed the question titles.\n",
    "\n",
    "> **_NOTE:_**  Api calls are rate limited to 3,500 requests per minute for paid users after 48 hours. That's just more than 58 per second. We'll limit the number of the threads in the threadpool to achieve this however in a production setting you should consider a more robust method of rate limiting to avoid rate limiting errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "84894ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 941510 tokens in the title column of the dataset\n",
      "Total cost of embeddings is $0.094151\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import numpy\n",
    "\n",
    "\n",
    "def count_tokens(text: str, model_name: str = \"text-embedding-ada-002\") -> int:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    tokens = enc.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "total_tokens = np.sum(df['title'].apply(count_tokens))\n",
    "model_cost = 0.0001 # text-embedding-ada-002 costs $0.0001 for 1,000 tokens\n",
    "\n",
    "print(f'There are {total_tokens} tokens in the title column of the dataset')\n",
    "print(f'Total cost of embeddings is ${total_tokens / 1000 * model_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0c0f9",
   "metadata": {},
   "source": [
    "Thankfully the cost is less than \\\\$0.10 but if we had tried to embed the full question descriptions we would be looking at a significantly higher bill (probably around \\\\$10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436d1208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85085it [00:01, 79177.49it/s]\n",
      "85085it [34:01, 41.67it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = parallel_embedding(df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d2331",
   "metadata": {},
   "source": [
    "# Efficient similarity search using faiss by Facebook\n",
    "Faiss contains several methods for similarity search including euclidean distance and dot product. We'd like to use cosine similarity, which happens to be the same as the dot product on normalised vectors. We'll then create our search index, normalise the embeddings and add them to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "03e522b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "\n",
    "def normalize(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "dimension = embeddings_normalized.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "embeddings_normalized = normalize(embeddings)\n",
    "index.add(embeddings_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f0c23824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(i, embeddings_normalized, n=10):\n",
    "    query_embedding = np.array([embeddings_normalized[i]])\n",
    "    # Search for the closest embeddings\n",
    "    distances, ind = index.search(query_embedding, n)\n",
    "    # Convert distances to cosine similarity\n",
    "    cosine_sim = 1 - distances / 2\n",
    "    # Return 1-d arrays\n",
    "    return ind[0], cosine_sim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e7d44a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24045</th>\n",
       "      <td>Bayesian vs frequentist Interpretations of Pro...</td>\n",
       "      <td>0.923964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7617</th>\n",
       "      <td>Interpretation of Bayesian vs Frequentist stat...</td>\n",
       "      <td>0.917631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75922</th>\n",
       "      <td>bayesian analysis of frequentist methods</td>\n",
       "      <td>0.911131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77335</th>\n",
       "      <td>Bayesian vs. frequentist estimation</td>\n",
       "      <td>0.910255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82312</th>\n",
       "      <td>Frequentist statistics</td>\n",
       "      <td>0.904825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55708</th>\n",
       "      <td>Am I understanding differences between Bayesia...</td>\n",
       "      <td>0.904304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36876</th>\n",
       "      <td>An example of the differences in frequentist a...</td>\n",
       "      <td>0.904229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24936</th>\n",
       "      <td>Bayesian and frequentist interpretations vs ap...</td>\n",
       "      <td>0.902857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77975</th>\n",
       "      <td>Bayesian vs. Frequentist calculation steps</td>\n",
       "      <td>0.902417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title       sim\n",
       "2      Bayesian and frequentist reasoning in plain En...  1.000000\n",
       "24045  Bayesian vs frequentist Interpretations of Pro...  0.923964\n",
       "7617   Interpretation of Bayesian vs Frequentist stat...  0.917631\n",
       "75922           bayesian analysis of frequentist methods  0.911131\n",
       "77335                Bayesian vs. frequentist estimation  0.910255\n",
       "82312                             Frequentist statistics  0.904825\n",
       "55708  Am I understanding differences between Bayesia...  0.904304\n",
       "36876  An example of the differences in frequentist a...  0.904229\n",
       "24936  Bayesian and frequentist interpretations vs ap...  0.902857\n",
       "77975         Bayesian vs. Frequentist calculation steps  0.902417"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "i = 2\n",
    "ind, sim = search(i, embeddings_normalized)\n",
    "pd.DataFrame(data={'title': df['title'][ind], 'sim': sim})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3727c",
   "metadata": {},
   "source": [
    "# Finding near duplicates\n",
    "Now we have an efficient way to search our embeddings using cosine similairty lets find the top 10 most similar titles, for each of our titles. We can then filter out any that do not meet a given similarity threshold. We'll then remove duplicate matches as well as any matches from one title to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "231ff7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def find_near_duplicates(embeddings_normalized, threshold, n=10):\n",
    "    arrays = []\n",
    "    similarities = []\n",
    "\n",
    "    for i in tqdm(range(len(embeddings_normalized))):\n",
    "        ind, sim = search(i, embeddings_normalized, n=n)\n",
    "        a = np.zeros((n, 2)).astype(int)\n",
    "        a[:, 0] = i\n",
    "        a[:, 1] = ind\n",
    "        arrays.append(a)\n",
    "        similarities.append(sim)\n",
    "\n",
    "    # Concatenate arrays and similarities\n",
    "    concatenated_arrays = np.vstack(arrays)\n",
    "    concatenated_similarities = np.concatenate(similarities)\n",
    "\n",
    "    # Filter out rows that do not meet the threshold\n",
    "    mask = concatenated_similarities >= threshold\n",
    "    concatenated_arrays = concatenated_arrays[mask]\n",
    "    concatenated_similarities = concatenated_similarities[mask]\n",
    "\n",
    "    # Remove duplicate matches and matches between the same items\n",
    "    mask = concatenated_arrays[:, 0] < concatenated_arrays[:, 1]\n",
    "    concatenated_arrays = concatenated_arrays[mask]\n",
    "    concatenated_similarities = concatenated_similarities[mask]\n",
    "\n",
    "    return concatenated_arrays, concatenated_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "374fb381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 85085/85085 [22:05<00:00, 64.17it/s]\n"
     ]
    }
   ],
   "source": [
    "a, sim = find_near_duplicates(embeddings_normalized, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7547e722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_left</th>\n",
       "      <th>ind_left</th>\n",
       "      <th>title_right</th>\n",
       "      <th>ind_right</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Posterior distribution</td>\n",
       "      <td>40323</td>\n",
       "      <td>Posterior distribution</td>\n",
       "      <td>69304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tests of within subjects contrasts in R</td>\n",
       "      <td>11685</td>\n",
       "      <td>Tests of within subjects contrasts in R</td>\n",
       "      <td>12957</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Priors and Loss in R</td>\n",
       "      <td>46962</td>\n",
       "      <td>Priors and Loss in R</td>\n",
       "      <td>69759</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Convert SAS syntax for mixed models for repeat...</td>\n",
       "      <td>68468</td>\n",
       "      <td>Convert SAS syntax for mixed models for repeat...</td>\n",
       "      <td>80367</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Convergence in distribution</td>\n",
       "      <td>24294</td>\n",
       "      <td>Convergence in distribution</td>\n",
       "      <td>28155</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>Multiple Regression Model vs Multivariate Model</td>\n",
       "      <td>51868</td>\n",
       "      <td>Multiple Versus Multivariate Regression</td>\n",
       "      <td>54461</td>\n",
       "      <td>0.950009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>Calculating probability</td>\n",
       "      <td>59367</td>\n",
       "      <td>How to compute probability</td>\n",
       "      <td>75141</td>\n",
       "      <td>0.950008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>Standard error of proportions, with weighting</td>\n",
       "      <td>33441</td>\n",
       "      <td>Standard error of proportion</td>\n",
       "      <td>46926</td>\n",
       "      <td>0.950007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>Interpreting ACF and PACF Plot</td>\n",
       "      <td>25207</td>\n",
       "      <td>Help to interpret unusual ACF/PACF plots</td>\n",
       "      <td>43558</td>\n",
       "      <td>0.950004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>Understanding convolutional neural networks</td>\n",
       "      <td>30083</td>\n",
       "      <td>What is a convolutional neural network</td>\n",
       "      <td>47721</td>\n",
       "      <td>0.950001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3752 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_left  ind_left  \\\n",
       "0                                Posterior distribution     40323   \n",
       "1               Tests of within subjects contrasts in R     11685   \n",
       "2                                  Priors and Loss in R     46962   \n",
       "3     Convert SAS syntax for mixed models for repeat...     68468   \n",
       "4                           Convergence in distribution     24294   \n",
       "...                                                 ...       ...   \n",
       "3747    Multiple Regression Model vs Multivariate Model     51868   \n",
       "3748                            Calculating probability     59367   \n",
       "3749      Standard error of proportions, with weighting     33441   \n",
       "3750                     Interpreting ACF and PACF Plot     25207   \n",
       "3751        Understanding convolutional neural networks     30083   \n",
       "\n",
       "                                            title_right  ind_right  similarity  \n",
       "0                                Posterior distribution      69304    1.000000  \n",
       "1               Tests of within subjects contrasts in R      12957    1.000000  \n",
       "2                                  Priors and Loss in R      69759    1.000000  \n",
       "3     Convert SAS syntax for mixed models for repeat...      80367    1.000000  \n",
       "4                           Convergence in distribution      28155    1.000000  \n",
       "...                                                 ...        ...         ...  \n",
       "3747            Multiple Versus Multivariate Regression      54461    0.950009  \n",
       "3748                         How to compute probability      75141    0.950008  \n",
       "3749                       Standard error of proportion      46926    0.950007  \n",
       "3750           Help to interpret unusual ACF/PACF plots      43558    0.950004  \n",
       "3751             What is a convolutional neural network      47721    0.950001  \n",
       "\n",
       "[3752 rows x 5 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = pd.DataFrame(\n",
    "    data = {\n",
    "        'title_left': df.title.values[a[:, 0]],\n",
    "        'ind_left': a[:, 0],\n",
    "        'title_right': df.title.values[a[:, 1]],\n",
    "        'ind_right': a[:, 1],\n",
    "        'similarity': sim,\n",
    "    }\n",
    ")\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d60ca",
   "metadata": {},
   "source": [
    "# Removing the duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131097e",
   "metadata": {},
   "source": [
    "Reviewing the 3,752 paired entries, we've identified a wealth of potential duplicate candidates. There is no doubt that cross-validated do a significant amount of work to ensure that duplicate questions are detected and removed so this dataset is already very clean.\n",
    "\n",
    "To go a step further we could add the long descriptions of questions into the embedding step to incorporate more semantic information, however, for the sake of this tutorial (and my wallet) I have kept things simple.\n",
    "\n",
    "As an additional step we could send this subset of results of for human evaluation or we could leverage a large language model to verify that duplicates are truly asking the same questions. For simplicity we'll assume that all the near-duplicates are true duplicates. Let's go ahead and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5ff4528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean index\n",
    "mask = np.ones(len(df)).astype(bool)\n",
    "#Â Mark the duplicates for removal\n",
    "mask[a[:, 1]] = False\n",
    "# Apply the mask\n",
    "df = df.loc[mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "174cd8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the meaning of p values and t values i...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Examples for teaching: Correlation does not me...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85079</th>\n",
       "      <td>85079</td>\n",
       "      <td>Modeling linear regression with covariate depe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85080</th>\n",
       "      <td>85080</td>\n",
       "      <td>Interpretation of global test (p-value) of the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85082</th>\n",
       "      <td>85082</td>\n",
       "      <td>How do I know a simple validation result is st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85083</th>\n",
       "      <td>85083</td>\n",
       "      <td>Kendall conditional independence test statistic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85084</th>\n",
       "      <td>85084</td>\n",
       "      <td>Heteroskedasticity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82463 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  score\n",
       "0          0  The Two Cultures: statistics vs. machine learn...    272\n",
       "1          1                     Forecasting demographic census      4\n",
       "2          2  Bayesian and frequentist reasoning in plain En...    208\n",
       "3          3  What is the meaning of p values and t values i...    138\n",
       "4          4  Examples for teaching: Correlation does not me...     58\n",
       "...      ...                                                ...    ...\n",
       "85079  85079  Modeling linear regression with covariate depe...      0\n",
       "85080  85080  Interpretation of global test (p-value) of the...      0\n",
       "85082  85082  How do I know a simple validation result is st...      1\n",
       "85083  85083    Kendall conditional independence test statistic      1\n",
       "85084  85084                                 Heteroskedasticity      0\n",
       "\n",
       "[82463 rows x 3 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
